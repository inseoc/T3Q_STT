{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12fc352-fef1-4ea2-9bb9-099729fc4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "해당 코드는 Postprocess.ipynb 파일 코드들을 압축한 것\n",
    "'''\n",
    "from tqdm import tqdm\n",
    "from konlpy.tag import Mecab\n",
    "from soynlp.hangle import jamo_levenshtein\n",
    "from kspon_preprocess import special_filter, bracket_filter\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6065f6-89d4-477b-a533-25fb4c2abe97",
   "metadata": {},
   "source": [
    "# vocab 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78333ed7-72df-448b-a789-f57f3d8aa7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "vocab에 입력할 단어 생성\n",
    "- 온갖 특수 기호 및 영어, 숫자, 퍼센트 등 한글 외의 데이터는 무시\n",
    "- 성능과 별개로, 속도를 위해 가장 빠른 정제 속도를 가진 mecab 을 사용\n",
    "- text_path_list : vocab으로 사용할 모든 텍스트 데이터들의 경로를 담은 자료형 변수\n",
    "'''\n",
    "mecab = Mecab()\n",
    "remove_re = '[a-zA-Z0-9%]'\n",
    "raw_texts = list()\n",
    "text_list = list()\n",
    "for texts in tqdm(text_path_list):\n",
    "    try:\n",
    "        with open(texts, 'r', encoding='cp949') as f:\n",
    "            text = f.read()\n",
    "    except:\n",
    "        with open(texts, 'r') as f:\n",
    "            text = f.read()\n",
    "    # ksponspeech 데이터셋 전용 정제 모듈인 special_filter, bracket_filter 을 사용\n",
    "    text = special_filter(bracket_filter(text))\n",
    "    if re.findall(remove_re, text) == []:\n",
    "        text_list.append(text)\n",
    "        text = mecab.pos(text)\n",
    "        for tt in text:\n",
    "            # mecab.pos를 통해 추출한 품사들 중 품사명의 가장 맨 앞(ex. tt[1].find(\"N\")==0) 부분이 N or V 일 경우, 명사, 형용사, 동사 판단하고 이들만 저장\n",
    "            if (tt[1].find(\"N\") == 0) or (tt[1].find(\"V\") == 0):\n",
    "                raw_texts.append(tt[0])\n",
    "\n",
    "raw_words = list(set(raw_texts))\n",
    "\n",
    "print(len(raw_words))\n",
    "print(raw_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f13960-8c8f-4a7b-991f-e4fe36ac381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(word_list:list) -> list:\n",
    "    '''\n",
    "    리스트에 포함된 모든 단어들을 dict 형태의 vocab으로 변환\n",
    "    '''\n",
    "    word_list.sort()\n",
    "    \n",
    "    vocab_dict = {}\n",
    "    for index, word in enumerate(word_list):\n",
    "        vocab_dict[word] = index\n",
    "        \n",
    "    return vocab_dict\n",
    "\n",
    "vocab_path = \"/wav2vec2/s-kr/fine-tune/transformer/post_vocab.json\"\n",
    "\n",
    "## vocab 파일 로드\n",
    "with open(vocab_path, 'r') as f:\n",
    "    origin_vocab_dict = json.load(f)\n",
    "\n",
    "## vocab 파일 생성\n",
    "# with open(vocab_path, 'w') as vocab_file:\n",
    "#     json.dump(vocab_dict, vocab_file)\n",
    "\n",
    "print(len(origin_vocab_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a1736-3409-4d07-9263-db869493d590",
   "metadata": {},
   "source": [
    "# vocab 동시등장행렬 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efa6976-63c4-47ba-9b0f-fbaf2b8685c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "데이터프레임으로 기본적인 동시등장행렬 생성\n",
    "vocab size에 따른 행, 열 개수 결정됨\n",
    "'''\n",
    "co_ocurrence_vectors = pd.DataFrame(\n",
    "    np.zeros([len(origin_vocab_dict), len(origin_vocab_dict)]),\n",
    "    index = origin_vocab_dict.keys(),\n",
    "    columns = origin_vocab_dict.keys()\n",
    ")\n",
    "\n",
    "co_ocurrence_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8049909f-d1fb-441c-8fda-2c343270e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = text_list\n",
    "\n",
    "'''\n",
    "- 11~12시간 소요..(vocab 54000개 기준)\n",
    "- vocab에 사용한 모든 텍스트 원문들을 다시 mecab.pos를 통해 명사, 형용사, 동사만 추출\n",
    "- co_ocurrence_vectors.loc를 통해 각 단어쌍이 등장하면 해당 값에 +1 을 해줌\n",
    "'''\n",
    "for page in tqdm(pages):\n",
    "    elements_list = [elements for elements in mecab.pos(page) if (elements[1].find(\"N\") == 0) or (elements[1].find(\"V\") == 0)]\n",
    "    for idx1, element1 in enumerate(elements_list):\n",
    "        for idx2, element2 in enumerate(elements_list):\n",
    "            # if idx1 < idx2:\n",
    "                try:\n",
    "                    co_ocurrence_vectors.loc[element1[0], element2[0]] = (co_ocurrence_vectors.loc[element1[0], element2[0]]+1)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "co_ocurrence_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc257f2a-b7bf-46b1-b1ca-a49aba58de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 약 10GB 크기의 csv 파일 생성(vocab 54000개 기준)\n",
    "co_ocurrence_vectors.to_csv(\"/wav2vec2/s-kr/fine-tune/transformer/co_occur_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb8532-838c-4604-89ed-4192c8130fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# co_ocurrence_vectors csv 파일 로드 / 대략 1시간 30분 정도 소요(vocab 54000개 기준)\n",
    "co_ocurrence_vectors = pd.read_csv(\"/wav2vec2/s-kr/fine-tune/transformer/co_occur_df.csv\")\n",
    "co_ocurrence_vectors = co_ocurrence_vectors.set_index(keys=['Unnamed: 0'], inplace=False, drop=True)\n",
    "co_ocurrence_vectors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455518c9-2d70-429a-a92e-f0703e851f27",
   "metadata": {},
   "source": [
    "# 추론 데이터셋 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c37d818-6e5c-4af1-adc3-2e9d4410b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_dataset_file = \"/wav2vec2/s-kr/fine-tune/transformer/Korean_corpus_2021.tsv\"\n",
    "\n",
    "re_string1 = '[^A-Za-z0-9가-힣]'\n",
    "re_string2 = '[^A-Za-z0-9가-힣 ]'\n",
    "\n",
    "df = pd.read_csv(infer_dataset_file, sep='\\t', index_col=0)\n",
    "\n",
    "df.drop(['id', 'original_form'], axis=1, inplace=True)\n",
    "test_df = df.copy()\n",
    "\n",
    "## lambda를 활용하여 모든 데이터프레임의 문장에서 영어, 숫자, 한글(음절)을 제외한 기호를 전부 제거\n",
    "test_df['form'] = test_df['form'].apply(lambda x: re.sub(re_string1, '', str(x)))\n",
    "test_df['corrected_form'] = test_df['corrected_form'].apply(lambda x: re.sub(re_string1, '', str(x)))\n",
    "## form과 corrected_form 데이터의 차이가 특수기호 유무로만 나뉘는 데이터들을 제외하기 위해 != 를 사용\n",
    "new_df = test_df[test_df['form'] != test_df['corrected_form']]\n",
    "## 너무 짧은 문장은 동시등장행렬로 구축할 때 수가 너무 적어 코사인 유사도 계산이 안되므로 최소 길이 4를 설정 후 다시 != 로 정제\n",
    "new_df = new_df.apply(lambda x: x if len(x) > 4 else \" \")\n",
    "new_df = new_df[new_df['form'] != new_df['corrected_form']]\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2762dab2-0096-49e4-a3cf-862cb061e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "- 정확하게 form과 corrected_form 데이터의 차이가 음절 오탈자로 인해 생긴 데이터만 남김\n",
    "- random을 자유롭게 사용함으로써 랜덤 추출을 할 지, 데이터를 지정할 지 결정 가능\n",
    "'''\n",
    "import random\n",
    "\n",
    "refine_idx = list(new_df.index)\n",
    "random.shuffle(refine_idx)\n",
    "refine_idx = refine_idx[:1000]\n",
    "## 원본 데이터 df에서 사용할 데이터셋을 refine_idx로 뽑아냄\n",
    "refine_df = df.loc[refine_idx]\n",
    "refine_df = refine_df.reset_index(drop=True)\n",
    "## 다시 한 번 쓸데없는 기호들을 제거\n",
    "refine_df['form'] = refine_df['form'].apply(lambda x: re.sub(re_string2, '', str(x)))\n",
    "refine_df['corrected_form'] = refine_df['corrected_form'].apply(lambda x: re.sub(re_string2, '', str(x)))\n",
    "## 빠른 추론 확인을 위해 우선 100개만 사용하기로 결정\n",
    "refine_df = refine_df[:100]\n",
    "refine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2186a77-2ac6-41eb-abb6-ce9d3ea337a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_text = list(refine_df['form'][0:50])\n",
    "correct_text = list(refine_df['corrected_form'][50:])\n",
    "## 추론에 사용할 데이터 리스트 생성 완료\n",
    "f1_text_list = error_text + correct_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236422c7-d0d2-444c-92ca-6878abfcfb05",
   "metadata": {},
   "source": [
    "# 추론 및 성능 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfbcf2b-2019-4052-9aac-033de7e3f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(a: list, b: list) -> float:\n",
    "    '''\n",
    "    입력받은 a,b 가 0 으로만 구성된 리스트일 경우,\n",
    "    norm(a) or norm(b) 연산 과정에서 0.0 이 나올 우려가 있으므로\n",
    "    이를 위해 0으로 구성된 리스트에게 0.1을 더하여 분모값이 0.0이 되는 것을 방지한다.\n",
    "    '''\n",
    "    try:\n",
    "        if (a == [.0] * len(a)) or (b == [.0] * len(b)):\n",
    "            raise RuntimeWarning\n",
    "        cosine = dot(a, b)/(norm(a)*norm(b))\n",
    "    except:\n",
    "        if (a == [.0] * len(a)) or (b != [.0] * len(b)):\n",
    "            cosine = dot(a, b)/((norm(a)+0.01)*(norm(b)))\n",
    "        elif (a != [.0] * len(a)) or (b == [.0] * len(b)):\n",
    "            cosine = dot(a, b)/(norm(a)*(norm(b)+0.01))\n",
    "        else:\n",
    "            cosine = dot(a, b)/((norm(a)+0.01)*(norm(b)+0.01))\n",
    "    finally:\n",
    "        return cosine\n",
    "    \n",
    "\n",
    "def co_occurence_matrix(input_elements, input_vector: pd.DataFrame, vocab_vector=co_ocurrence_vectors: pd.DataFrame) -> pd.DataFrame:\n",
    "    for element1 in input_elements:\n",
    "        for element2 in input_elements:\n",
    "            try:\n",
    "                input_vector.loc[element1, element2] = vocab_vector.loc[element1, element2]\n",
    "            except:\n",
    "                input_vector.loc[element1, element2] = 0\n",
    "    \n",
    "    return input_vector\n",
    "\n",
    "\n",
    "def refine_ed_word(typo_word_list: list, vocab_dict=origin_vocab_dict: dict, ed_score=0.5: float) -> dict:\n",
    "    ed_dict = dict()\n",
    "    vocab_list = list(vocab_dict.keys())\n",
    "    for typo in typo_word_list:\n",
    "        ed_list = list()\n",
    "        for vocab in vocab_list:\n",
    "            ed_score = jamo_levenshtein(typo, vocab)\n",
    "            # 편집거리 임계치 설정\n",
    "            if ed_score < 0.5:\n",
    "                ed_list.append(vocab)\n",
    "        # 각 오탈자 별 대체 단어 후보들을 입력\n",
    "        ed_dict[typo] = ed_list\n",
    "    \n",
    "    return ed_dict\n",
    "    \n",
    "    \n",
    "output_text_list = list()\n",
    "error_idx = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2eba75-7a10-4ddc-9216-694e07fcc4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tp_idx, tp_text in enumerate(tqdm(f1_text_list)):\n",
    "    ## 오탈자 문장마다 명사, 동사, 형용사 단어만 추출\n",
    "    input_elements = [elements[0] for elements in mecab.pos(tp_text) if (elements[1].find(\"N\") == 0) or (elements[1].find(\"V\") == 0)]\n",
    "    ## 각 문장마다 동시등장행렬 생성을 위해 입력받은 문장으로 vocab 생성\n",
    "    vocab_dict = build_vocabulary(input_elements)\n",
    "    ## 입력받은 문장에 대한 base 행렬을 생성\n",
    "    input_co_ocur_vector = pd.DataFrame(\n",
    "        np.zeros([len(vocab_dict), len(vocab_dict)]),\n",
    "        index = vocab_dict.keys(),\n",
    "        columns = vocab_dict.keys()\n",
    "    )\n",
    "    ## loc를 활용하여 단어쌍이 등장하는 횟수만큼 1씩 덧셈하며 단어쌍이 vocab 동시등장행렬에 없을 경우는 그냥 0 을 입력\n",
    "    input_co_ocur_vector = co_occurence_matrix(input_elements, input_co_ocur_vector)\n",
    "\n",
    "    ## 단어별 코사인 유사도를 계산 후 평균값을 계산, 단 같은 값끼리는 의미가 없으므로 != 을 통해 pass 하도록 한다\n",
    "    typo_word = list()\n",
    "    input_co_occur_list = input_co_ocur_vector.values.tolist()\n",
    "    for idx, input_list1 in enumerate(input_co_occur_list):\n",
    "        if len(input_co_occur_list) > 1:\n",
    "            total_cosine = 0.0\n",
    "            for input_list2 in input_co_occur_list:\n",
    "                if input_list1 != input_list2:\n",
    "                    total_cosine += cos_sim(input_list1, input_list2)\n",
    "\n",
    "                # input_list1 에 대한 평균 코사인 유사도를 계산\n",
    "            avg_cosine = total_cosine / (len(input_list1) - 1)\n",
    "                # 코사인 유사도 임계치 설정\n",
    "            if avg_cosine < 0.20:\n",
    "                # typo_word 에 입력된 단어는 '오탈자'로 확정 인식\n",
    "                typo_word.append(input_co_ocur_vector.index[idx])\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    ## 모든 vocab 과 입력받은 문장의 단어끼리 하나하나 편집거리 계산\n",
    "    ed_dict = refine_ed_word(typo_word)\n",
    "\n",
    "    ## 이후 오탈자를 대체 단어로 바꿔주면서 동시등장행렬, 코사인 유사도, 편집거리 계산을 동일하게 진행\n",
    "    results_list = list()\n",
    "    for origin_word in list(ed_dict.keys()):\n",
    "        og_words = list()\n",
    "        replace_words = list()\n",
    "        cosine_value = list()\n",
    "\n",
    "        for ed_word in ed_dict[origin_word]:\n",
    "            # 오탈자 컬럼명을 대체 단어 컬럼명 및 인덱스명으로 교체\n",
    "            ed_vector = input_co_ocur_vector.rename(columns={origin_word: ed_word}, index={origin_word: ed_word}, inplace=False)\n",
    "            # 데이터 값들 전부 0으로 리셋\n",
    "            ed_vector = pd.DataFrame(\n",
    "                np.zeros([len(ed_vector), len(ed_vector)]),\n",
    "                index = ed_vector.index,\n",
    "                columns = ed_vector.columns\n",
    "            )\n",
    "\n",
    "            ed_vector = co_occurence_matrix(ed_vector.columns, ed_vector)\n",
    "\n",
    "            ed_co_occur_list = ed_vector.values.tolist()\n",
    "            total_cosine = 0.0\n",
    "            ed_idx = list(ed_vector.columns).index(ed_word)\n",
    "            for input_list1 in ed_co_occur_list:\n",
    "                if ed_co_occur_list[ed_idx] != input_list1:\n",
    "                    total_cosine += cos_sim(ed_co_occur_list[ed_idx], input_list1)\n",
    "\n",
    "            avg_cosine = total_cosine / (len(ed_co_occur_list[ed_idx]) - 1)\n",
    "            # 기존 오탈자, 대체 단서, 평균 코사인 유사도 계산 값을 각각 리스트에 넣는다\n",
    "            og_words.append(origin_word)\n",
    "            replace_words.append(ed_word)\n",
    "            cosine_value.append(avg_cosine)\n",
    "        if cosine_value != []:\n",
    "        # 가장 유사도가 높은 값이 오탈자를 대체할 단어로 지정된다\n",
    "            cs_idx = cosine_value.index(max(cosine_value))\n",
    "            rp_word = replace_words[cs_idx]\n",
    "            # 최종적으로 오탈자와 확정된 대체 단어를 리스트에 같이 입력\n",
    "            results_list.append([origin_word, rp_word])\n",
    "            \n",
    "    if results_list != []:\n",
    "    ## 각 오탈자를 대체 단어로 교체하면서 결과 텍스트를 최종적으로 리스트에 입력한다.\n",
    "        output_text = tp_text\n",
    "        for results in results_list:\n",
    "            output_text = output_text.replace(results[0], results[1])\n",
    "        output_text_list.append(output_text)\n",
    "        # 성능 확인을 위해 오탈자 수정 작업이 이뤄진 인덱스 위치값을 저장\n",
    "        error_idx.append(tp_idx)\n",
    "    else:\n",
    "        output_text_list.append(tp_text)\n",
    "\n",
    "print(len(output_text_list))\n",
    "print(output_text_list[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
